Algorithmic Extremism: Examining YouTube’s Rabbit Hole of Radicalization

Mark Ledwich Brisbane, Australia mark@ledwich.com.au

Anna Zaitsev The School of Information University of California, Berkeley
Berkeley, United States anna.zaitsev@ischool.berkeley.edu

arXiv:1912.11211v1 [cs.SI] 24 Dec 2019

Abstract—The role that YouTube and its behind-the-scenes recommendation algorithm plays in encouraging online radicalization has been suggested by both journalists and academics alike. This study directly quantiﬁes these claims by examining the role that YouTubes algorithm plays in suggesting radicalized content. After categorizing nearly 800 political channels, we were able to differentiate between political schemas in order to analyze the algorithm trafﬁc ﬂows out and between each group. After conducting a detailed analysis of recommendations received by each channel type, we refute the popular radicalization claims. To the contrary, these data suggest that YouTubes recommendation algorithm actively discourages viewers from visiting radicalizing or extremist content. Instead, the algorithm is shown to favor mainstream media and cable news content over independent YouTube channels with slant towards left-leaning or politically neutral channels. Our study thus suggests that YouTubes recommendation algorithm fails to promote inﬂammatory or radicalized content, as previously claimed by several outlets.
Index Terms—YouTube, recommendation algorithm, radicalization
I. INTRODUCTION
The internet can both be a powerful force for good, prosocial behaviors by providing means for civic participation and community organization [1], as well as an attractor for antisocial behaviors that create polarizing extremism [2]. This dual nature of the internet has been evident since the early days of online communication, where ”ﬂame-wars” and ”trolling” have been present in online communities for over two decades [3] [4] [5]. While such behaviors were previously conﬁned to Usenet message boards and limited IRC channels, with the expansion of social media, blogs, and microblogging following the rapid growth of internet participation rates, these inﬂammatory behaviors are no longer conﬁned and have left their early back-channels into public consciousness [6].
The explosion of platforms, as well as ebbs and ﬂows in the political climate, has exacerbated the prevalence of antisocial messaging [7]. Research focusing on uninhibited or antisocial communication, as well as extremist messaging online has previously been conducted on platforms including Facebook [8], Twitter [9], Reddit [10], 4chan and 8chan [11] [12], Tumblr [13] and even knitting forums such as Ravelry [14].
In addition to these prior studies on other platforms, attention has recently been paid to the role that YouTube may play as a platform for radicalization [15] [16] [17]. As a content host, YouTube provides a great opportunity for

broadcasting a large and widely diverse set of ideas to millions of people worldwide. Included among general content creators are those who speciﬁcally target users with polarizing and radicalizing political content. While YouTube and other social media platforms have generally taken a strict stance against most inﬂammatory material on their platform, extremist groups from jihadi terrorist organizations [18] [19], various political positions [20], and conspiracy theorists have nonetheless been able to permeate the content barrier [21].
Extreme content exists on a spectrum. YouTube and other social media platforms have generally taken a strict stance against the most inﬂammatory materials or materials that are outright illegal. No social media platform tolerates ISIS beheading videos, child porn, or videos depicting cruelty towards animals. There seems to a consensus amongst all social media platforms that human moderators or moderation algorithms will remove this type of content [22].
YouTube’s automatic removal of the most extreme content, such as explicitly violent acts, child pornography, and animal cruelty, has created a new era of algorithmic data mining [23] [24] [13]. These methods range from metadata scans [25] to sentiment analysis [26]. Nevertheless, content within an ideological grey area or that can nonetheless be perceived as ”radicalizing” exists on YouTube [27]. Deﬁnitions of free speech differ from country to country. However, YouTube operates on a global scale within the cultural background of the United States with robust legislation that protects speech [7]. Even if there are limitations to what YouTube will broadcast, the platform does allow a fair bit of content that could be deemed as radicalizing, either by accident or by lack of monitoring resources.
Means such as demonetization, ﬂagging, or comment limiting is several tools available to content moderators on YouTube [28]. Nevertheless, removing or demonetizing videos or channels that present inﬂammatory content has not curtailed scrutiny of YouTube by popular media [29]. Recently, the New York Times published a series of articles, notably critiquing YouTube’s recommendation algorithm, which suggests related videos for users based on their prior preferences and users with similar preferences [30] [15]. The argument put forward by the NYT is that users would not otherwise have stumbled upon extremist content if they were not actively searching for it since the role of recommendation algorithms for content on

other websites is less prevalent. As such, YouTube’s algorithm may have a role in guiding content, and to some extent, preferences towards more extremist predispositions. Critical to this critique is that while previous comments on the role that social media websites play in spreading radicalization have focused on user contributions, the implications of the recommendation algorithm strictly implicate YouTube’s programming as an offender.
The critique of the recommendation algorithm is another difference that sets YouTube apart from other platforms. In most cases, researchers are looking at how the users apply social media tools as ways to spread jihadism [18], alt-right messages of white supremacy [12]. Studies are also focusing on the methods the content creators might use to recruit more participants in various movements; for example, radical leftwing Antifa protests [31]. Nevertheless, the premise is that users of Facebook, Tumblr, or Twitter would not stumble upon extremists if they are not actively searching for it since the role of recommendation algorithms is less prevalent. There are always some edge cases where innocuous Twitter hashtags can be co-opted for malicious purposes by extremists or trolls [19], but in general, users get what they speciﬁcally seek. However, the case for YouTube is different: the recommendation algorithm is seen as a major factor in how users engage with YouTube content. Thus, the claims about YouTube’s role in radicalization are twofold. First, there are content creators that publish content that has the potential to radicalize [15]. Second, YouTube is being scrutinized for how and where the recommendation algorithm directs the user trafﬁc [17] [15]. Nevertheless, empirical evidence of YouTube’s role in radicalization is insufﬁcient [32]. There are anecdotes of a radicalization pipeline and hate group rabbit hole, but academic literature on the topic is scant, as we discuss in the next section.
II. PRIOR ACADEMIC STUDIES ON YOUTUBE RADICALIZATION
Data-drive papers analyzing radicalization trends online are an emerging ﬁeld of inquiry. To date, few notable studies have examined YouTube’s content in relation to radicalization. As discussed, previous studies have concentrated on the content itself and have widely proposed novel means to analyze these data [13] [33] [25]. However, these studies focus on introducing means for content analysis, rather than the content analysis itself.
However, a few studies go beyond content analysis methods. One such study, Ottoni et al. (218), analyzed the language used in right-wing channels compared to their baseline channels. The study concludes that there was little bias against immigrants or members of the LGBT community, but there was limited evidence for prejudice towards Muslims. However, the study did ﬁnd evidence for the negative language used by channels labeled as right-wing. Nevertheless, this study has a few weaknesses. The authors of this paper frame their analysis as an investigation into right-wing channels but then proceed to analyze kooky conspiracy channels instead of

more mainstream right-wing content. They have chosen a conspiracy theorist Alex Jones’ InfoWars (nowadays removed from YouTube) as their seed channel, and their list of rightwing channels reﬂects this particular niche. InfoWars and other conspiracy channels represent only a small segment of right-wing channels. Besides, the study applies a topic analysis method derived from the Implicit Association Test (IAT) [34]. However, the validity of IAT has been contested [35]. In conclusion, we consider the seed channel selection as problematic and the range of the comparison channels as too vaguely explained [36].
In addition to content analysis of YouTube’s videos, Riberio et al. (2019) took a novel approach by analyzing the content of video comment sections, explaining which types of videos individual users were likely to comment on overtime. Categorizing videos in four categories, including alt-right, alt-light, the intellectual dark web (IDW), and a ﬁnal control group, the authors found inconclusive evidence of migration between groups of videos. 1
The analysis shows that a portion of commenters does migrate from IDW videos to the alt-light videos. There is also a tiny portion of commenter migration from the centrist IDW to the potentially radicalizing alt-right videos. However, we believe that one cannot conclude that YouTube is a radicalizing force based on commenter trafﬁc only. There are several ﬂaws in the setting of the study. Even though the study is commendable, it is also omitting the migration from the center to the left-of-center altogether, presenting a somewhat skewed view of the commenter trafﬁc. In addition, only a tiny fraction of YouTube viewers engage in commenting. For example, the most popular video by Jordan Peterson, a central character of the IDW, has 4.7 million views but only ten thousand comments. Besides, commenting on a video does not necessarily mean agreement with the content. A person leaving a comment on a controversial topic might stem from a desire to get a reaction (trolling or ﬂaming) from either the content creator or other viewers [37] [5]. We are hesitant to draw any conclusions based on the commenter migration without analyzing the content of the comments.
The most recent study by Munger and Phillips (2019) directly analyzed YouTube’s recommendation algorithm and suggested that the algorithm operated on a simple supplyand-demand principle. That is, rather than algorithms driving viewer preference and further radicalization, further radicalization external to YouTube inspired content creators to produce more radicalized content. The study furthermore failed to ﬁnd support for radicalization pathways, instead of ﬁnding that
1The study borrows a deﬁnition for the alt-right from Anti-Defamation League: ”loose segment of the white supremacist movement consisting of individuals who reject mainstream conservatism in favor of politics that embrace racist, anti-Semitic and white supremacist ideology” (pp. 2 [32]). The alt-light is deﬁned to be a civic nationalist group rather than racial nationalism groups. The third category, ”intellectual dark web” (IDW), is deﬁned as a collection of academics and podcasters who engage in controversial topics. The fourth category, the control group, includes a selection of channels form fashion magazine channels such as the (Cosmopolitan and GQ Magazine) to a set of left-wing and right-wing mainstream media outlets.

the growth belonging to the centrist IDW category reﬂected a deradicalization trend rather than further radicalization. Nevertheless, these authors are critical towards claims that watching content on Youtube will lead to the spread of radical ideas like a ”zombie bite” and are further critical of the potential pipeline from moderate, centrist channels to radical right-wing content.
III. ANALYZING THE YOUTUBE RECOMMENDATION ALGORITHM
Our study focuses on the YouTube recommendation algorithm and the direction of recommendations between different groups of political content. To analyze the common claims from media and other researchers, we have distilled them into speciﬁc claims that can be assessed using our data set. C1 - Radical Bubbles. Recommendations inﬂuence viewers of radical content to watch more similar content than they would otherwise, making it less likely that alternative views are presented. C2 - Right-Wing Advantage. YouTube’s recommendation algorithm prefers right-wing content over other perspectives. C3 - Radicalization Inﬂuence. YouTube’s algorithm inﬂuences users by exposing them to more extreme content than they would otherwise seek out. C4 - Right-Wing Radicalization Pathway. YouTube algorithm inﬂuences viewers of mainstream and center-left channels by recommending extreme right-wing content, content that aims to disparage left-wing or centrist narratives.
By analyzing whether the data supports these claims, we will be able to draw preliminary conclusions on the impact of the recommendation algorithm.
A. YouTube Channel Selection Criteria
The data for this study is collected from two sources. First, YouTube offers a few tools for software developers and researchers. Our research applies an application programming interface (API) that YouTube provides for other websites that integrate with YouTube and also for research purposes to deﬁne the channel information, including view and engagement statistics and countries. However, the YouTube API limited the amount of information we could retrieve and the period it could be kept and was thus not entirely suitable for this study. For this reason, we use an additional scraping algorithm that provides us information on individual video statistics such as views, likes, video title, and closed captions. This algorithm offers data since the ﬁrst of January, 2018. The scraping algorithm also provides us the primary data applied for this study: the recommendations that YouTube’s recommendation algorithm offers for each video. The scraping process runs daily.
The scraped data, as well as the YouTube API, provides us a view of the recommendations presented to an anonymous account. In other words, the account has not ”watched” any videos, retaining the neutral baseline recommendations, described in further detail by YouTube in their recent paper that

explains the inner workings of the recommendation algorithm [38]. One should note that the recommendations list provided to a user who has an account and who is logged into YouTube might differ from the list presented to this anonymous account. However, we do not believe that there is a drastic difference in the behavior of the algorithm. Our conﬁdence in the similarity is due to the description of the algorithm provided by the developers of the YouTube algorithm [38]. It would seem counter-intuitive for YouTube to apply vastly different criteria for anonymous users and users who are logged into their accounts, especially considering how complex creating such a recommendation algorithm is in the ﬁrst place. The study includes eight hundred and sixteen (816) channels which fulﬁll the following criteria:
• Channel has over ten thousand subscribers. • More than 30 percent of the content on the channel is
political.
The primary channel selection was made based on the number of subscriptions. The YouTube API provides channel details, including the number of subscribers and aggregate views of all time on the channel. The sizes of the bubble are based on the video views in the year 2018m, not the subscriber counts. YouTube also provides detailed info on the views of each video and dislikes, thus providing information on the additional engagement each video receives from the users.
Generally, only channels that had over ten thousand subscriptions were analyzed. However, if the channel’s subscription numbers were lower than our threshold value or there were missing data. However, if the channel is averaging over ten thousand views per month, the channel was still included.
We based our selection criteria on the assumption that tiny channels with minimal number of views or subscriptions are unlikely to fulﬁll YouTube’s recommendation criteria: ”1) engagement objectives, such as user clicks, and degree of engagement with recommended videos; 2) satisfaction objectives, such as user liking a video on YouTube, and leaving a rating on the recommendation [38].”
Another threshold for the channels was the focus of the content: only channels where more than 30 percent of the content was on US political or cultural news or cultural commentary, were selected. We based the cultural commentary selection on a list of social issues on the website ISideWith. A variety of qualitative techniques compiled the list of these channels.
The lists provided by Ad Fontes Media provides a starting point for the more mainstream and well-known alternative sites. Several blogs and other websites further list political channels or provide tools for advanced searches based on topics [39] [40] [41]. We also analyzed the recent academic studies and their lists of channels such as Ribero et al. (2019) and Munger and Philips (2019). However, not all channels included in these two studies ﬁt our selection criteria. Thus one can observe differences between the channel lists and categories between our research and other recent studies on a similar subject.

We added emerging channels by following the YouTube recommendation algorithm, which suggests similar content and which ﬁt the criteria and passed our threshold. We can conceptualize the recommendation algorithm as a type of snowball sampling, a common technique applied in social sciences when one is conducting interview-based data collection but also in the analysis of social networks. Each source is ”requested” to nominate a few candidates that would be of interest to the study. The researcher follows there recommendations until the informants reveal no new information or the inclusion criteria are met (e.g., channels become too marginal, or content is not political). In our case, there is a starting point; a channel acts as a node in the network. Each connected channel (e.g., node) in the network is visited. Depending on the content of the channel, it is either added to the collection of channels or discarded. Channels are visited until there are no new channels, or the new channels do not ﬁt the original selection criteria [42].
B. The Categorization Process
The categorization of YouTube channels was a non-trivial task. Activist organizations provide lists and classiﬁcations, but many of them are unreliable. For example, there are several controversies around the lists of hate groups discussed by the Southern Poverty Law Center (SPLC) [43]. Also, there seems to be a somewhat contentious relationship between the AntiDefamation League and YouTubers [44] [45]. We decided to create our categorization, based on multiple existing sources.
First, one has several resources to categorize mainstream or alternative media outlets. Mainstream media such as CNN or Fox News have been studied and categorized over time by various outlets [46] [47]. In our study, we applied two sites that provide information on the political views of mainstream media outlets: Ad Fontes Media and Media Bias Factcheck. Neither website is guaranteed to be unbiased, but by crossreferencing both, one can come to a relatively reliable categorization on the political bias of the major news networks. These sites covered the ﬁfty largest mainstream channels, which make up for almost 80 percent of all YouTube views.
Nevertheless, the majority of the political YouTube channels were not included in sources categorizing mainstream outlets. After reviewing the existing literature on political YouTube and the categorization created by authors such as Ribero et al. (2019) or Munger and Philips (2019), we decided to create a new categorization. Our study strives for a granular and precise classiﬁcation to facilitate a deep dive into the political subcultures of YouTube, and the extant categories were too narrow in their scope. We decided to apply on both a high-level left-center-right political classiﬁcation for highlevel analysis and create a more granular distinction between eighteen separate labels, described shortly in Table I or at length in Appendix A-D).
In addition to these ’soft tags,’ we applied a set of so-called ’hard tags.’ These additional tags allowed us to differentiate between YouTube channels that were part of mainstream media outlets and independent YouTubers. The hard tags

TABLE I CATEGORIZATION SOFT TAGS AND EXAMPLES

Tag Conspiracy A channel that regularly promotes a variety of conspiracy theories.
Libertarian Political philosophy with liberty as the main principle.
Anti-SJW Have a signiﬁcant focus on criticizing ”Social Justice” (see next category) with a positive view of the marketplace of ideas and discussing controversial topics. Social Justice Promotes identity Politics and intersectionality White Identitarian Identiﬁes-with/is-proud-of the superiority of ”whites” and western civilization.
Educational Channel that mainly focuses on education material. Late Night Talk shows Channel with content presented humorous monologues about the daily news.
Partisan Left Focused on politics and exclusively critical of Republicans. Partisan Right Channel mainly focused on politics and exclusively critical of Democrats, supporting Trump. Anti-theist Self-identiﬁed atheist who are also actively critical of religion. Religious Conservative A channel with a focus on promoting Christianity or Judaism in the context of politics and culture. Socialist (Anti-Capitalist) Focus on the problems of capitalism. Revolutionary Endorses the overthrow of the current political system.
Provocateur Enjoys offending and receiving any kind of attention. MRA (Mens Rights Activist) Focus on advocating for rights for men. Missing Link Media Channels not large enough to be considered ”mainstream.” State Funded Channels funded by governments.
Anti-Whiteness A subset of Social Justice that in addition to intersectional beliefs about race

Examples X22Report, The Next News Network Reason, John Stossel, The Cato Institute Sargon of Akkad, Tim Pool

Peter Cofﬁn,

hbomberguy

NPIRADIX

(Richard

Spencer)

TED,

SoulPancake

Last

Week

Tonight, Trevor

Noah

The

Young

Turks, CNN

Fox News, Can-

dace Owens

CosmicSkeptic, Matt Dillahunty Ben Shapiro, PragerU

Richald Wolf, NonCompete Libertarian Socialist Rants, Jason Unruhe StevenCrowder, MILO Karen Straughan

Vox, NowThis News PBS NewsHour, Al Jazeera, RT African Diaspora News Channel

are discussed in more detail in Appendix A. The difference between ’soft’ and ’hard’ tags is that hard tags were based on external sources, whereas the soft tags were based on the content analysis of the labelers.
The tagging process allowed each channel to be characterized by a maximum of four different tags to create meaningful and fair categories for the content. In addition to labeling created by the two authors, we recruited an additional volunteer labeler, who was well versed in the YouTube political sphere, and whom we trusted to label channels by their existing content accurately. When two or more labelers deﬁned a channel by the same label, that label was assigned to the channel. When the labelers disagreed and ended in a draw situation, the tag was not assigned. The majority was needed for a tag to be applied.
The visual analysis in Figure 1 shows the intraclass corre-

lation coefﬁciency (ICC) between the three labelers. Based on this analysis, we can determine that all three labelers were in agreement when it comes to the high-level labels, e.g., left-right-center. Besides, there is a high coefﬁciency in the majority of the granular categories. On the left side of the graph, we can see the intraclass correlation coefﬁciency values, the estimates of the ”real” information captured by our classiﬁcation, which ranges from 0 to one. The larger the number, the more similar the tags were. One the right side of the Figure, we see the reviewer agreement in percentages.
The ICC values above 0.75 are considered excellent, between 0.75 and 0.59 are good and above 0.4 are considered as fair [48]. In our categorization, few classiﬁcations measure under 0.4. However, we believe that the explanation for this convergence is related to the nature of these categories. The low coefﬁciency scoring of groups,’Provocateur’, ’Anti-whiteness’ and ”Revolutionary,’ could be explained by the labeler’s hesitation to apply these rather extreme labels where consistent evidence was lacking. Besides, since each channel was allowed four different ’soft tags’ deﬁning these subcategories, the channels were likely tagged by the other, milder tags. The rationale behind the lack of agreement on the ’Educational’ label is best explained by the fact that this category classiﬁcation might be somewhat superﬂuous. Political content, even educational one, often has a clear bias, and the content already belongs to one or more stronger categories, such as Partisan Left or to channels that are nonpolitical.
Fig. 1. The intraclass correlation coefﬁciency between the three labelers
However, if one looks at the percentages of the agreement, the agreement if very high in most cases. The only category

where disagreement seems to be signiﬁcant is the left-rightcenter categorization. However, this disagreement can be explained by the weighing applied when calculating the ICC factor.
To assign a label, we investigated which topics the channels discussed and from which perspective. Some channels are overtly partisan or declare their political stances and support for political parties in their introductions or have posted several videos where such topics are discussed. For example, libertarian channels support Ron and Rand Paul (Libertarian politicians afﬁliated with the Republican party) or discuss Austrian economics with references to economists such as Frederick von Hayek or Ludwig von Mises or the ﬁctional works of the author Ayn Rand. Comparably, many channels dedicated to various social justice issues title their videos to reﬂect the content and the political slant, e.g., ”Can Our Planet Survive Capitalism” or ”The Poor Go To Jail And The Rich Make Bail In America” from AJ+.
Nevertheless, other channels are more subtle and required more effort to tease out their afﬁliation. In these cases, we analyzed the perspective that these channels took on political events that have elicited polarized opinions (for example, the nomination of Brett Kavanaugh in the U.S. Supreme Court, the Migrant Caravan, Russiagate). Similarly, we also analyzed the reactions that the channels had for polarizing cultural events or topics (e.g., protests at university campuses, trans activism, free speech). If the majority of these considerations aligned in the same direction, then the channel was designated as leftleaning or right-leaning. If there was a mix, then the channels were likely assigned to the centrist category.
The only way to conduct this labeling was to watch the content on the channels until the labelers found enough evidence for assigning speciﬁc labels. For some channels, this was relatively straightforward: the channels had introductory videos that stated their political perspectives. Some of the intros are very clearly indicating the political viewers of the content creator; some are more subtle. For example, a political commentator Kyle Kulinski explicitly states his political leanings (libertarian-left) in channel SecularTalk description. In contrast, a self-described Classical Liberal discussion host Dave Rubin has a short introduction of various guests, providing examples of the political discussions that take place on his channel The Rubin Report. In other cases, the labelers could not assign a label based on introduction or description but had to watch several videos on the channel to determine the political leanings. On average, every labeler watched over 60 hours of YouTube videos to deﬁne the political leanings without miscategorizing the channel and thus misrepresenting the views of the content creators.
Based on the eighteen classiﬁcation categories, we created thirteen aggregate groups that broadly represent the political views of the YouTube channels. The eighteen ’soft tags’ were aggregated from ideological groups and better differentiated between the channels. For more details on tagging aggregation, please see the Appendix A-B. These groupings were applied in the data visualization rather than the more granular eighteen

categories for clarity and differentiation purposes. The next section will discuss the data in more detail.
IV. FINDINGS AND DISCUSSION The data on YouTube channels a viewership each channel garners provides us with insights as to how the recommendation algorithm operates. Per the data collected for 2019, YouTube hosted more channels with content that could be considered right-wing than before. In deﬁning right-wing, we considered categories such as proactive ”Anti-SJW” (for anti-Social Just Warrior, a term describing feminist/intersectionality advocates), PartisanRight, Religious Conservative, and to some extent Conspiracy Channels (for brief explanations, see Table I). For longer descriptions on the labels, see Appendix A). However, these more numerous channels gained only a fraction of the views of mainstream media and centrist channels. Categories such as the Center/Left MSM category, Unclassiﬁed category (consisting mainly of centrist, non-political and educational channels), and Partisan Left, capture the majority of viewership. The difference here is considerable: where Center/left MSM has 22 million daily views, the largest non-mainstream category, Anti-SJW, has 5.6 million daily views. Figure 2 illustrates the number of views for each category compared to the number of channels.
2
Fig. 2. Daily Views and Number of Channels
Figure 3 presents a chart of channel relations illustrating relations between channels and channel clusters based on the concept of a force-directed graph [49]. The area of each bubble, but not the radius, corresponds to the number of views a channel has. The force/size of the line links between channels corresponds to the portion of recommendations between these channels. From this chart, we can see left-wing and centrist mainstream media channels are clustered tightly together. The Partisan Right cluster is also closer to the large mainstream media cluster than it is to any other category. AntiSJW and Provocative Anti-SJW are clustered tightly together
2The Figure 2 and all the following Figures are applying the aggregated categories rather than the granular labels show in Figure 1 and discussed in Appendix A-B.

with libertarian channels, while smaller categories such as Anti-theists and socialists are very loosely linked to a limited number of other categories. White Identitarian channels are small and dispersed across the graph.
Fig. 3. Channel Clusters
When analyzing the recommendation algorithm, we are looking at the impressions the recommendation algorithm provides viewers of each channel.
By impressions, we are referring to an estimate for the number of times a viewer was presented with a speciﬁc recommendation. This number is an estimate because only YouTube is privy to the data reﬂecting granulated impressions. However, public-facing data obtained from channels themselves provide us with information on at least the top ten recommendations. A simpliﬁed formula for calculating the number of impressions from Channel A to Channel B is calculated by dividing the number of recommendations from A to B by the number of total recommendations channel A receives summed with channel views and recommendations per video, multiplied by ten (for further information, see Appendix A-A). Such a calculation of impressions allows us to aggregate the data between channels and categories.
Figure 4 presents the recommendation algorithm in a ﬂow diagram format. The diagram shows the seed channel categories on the left side and the recommendation channel categories on the left side. The sizes of channel categories are based on overall channel view counts. The fourth category from the top is the most viewed channel category, the Center/Left Mainstream media category (MSM). This group is composed of late-night talk shows, mainstream media shows, including the New York Times’ YouTube channel. The Partisan Left category closely follows the Center/Left MSM category, with the primary differentiating factor being that the Partisan Left category includes the content of independent YouTube creators. Together, these two most viewed categories garner close to forty million daily views.

Several smaller categories follow the top two-categories. Notably, the two-second largest categories are also centrist or left-leaning in their political outlook. For example, the two largest channels in the Anti-SJW category (JRE Clips and PowerfulJRE)) both belong to an American podcast host, Joe Rogan, who hosts guests from a wide range of political beliefs. The Unclassiﬁed groups consist of centrist, mostly apolitical, educational channels such as TED or governmentowned mainstream media channels such as Russia Today. Based on our ﬂow diagram, we can see that the recommendation algorithm directs trafﬁcs from all channel groups into the two largest ones, away from more niche categories.
Fig. 4. Flow diagram presenting the ﬂow or recommendations between different groups
Based on these data, we can now evaluate the claims that the YouTube recommendation algorithm will recommend content that contributes to the radicalization of YouTube’s user base. By analyzing each radicalization claim and whether the data support these claims, we can also conclude whether the YouTube algorithm has a role in political radicalization.
The ﬁrst claim tested is that YouTube creates C1 - Radical Bubbles., i.e., recommendations inﬂuence viewers of radical content to watch more similar content than they would otherwise, making it less likely that alternative views are

presented. Based on our data analysis, this claim is partially supported. The ﬂow diagram presented in Figure 4 shows a high-level view of the intra-category recommendations. The recommendations provided by the algorithm remain within the same category or categories that bear similarity to the original content viewed by the audience. However, from the ﬂow diagram, one can observe that many channels receive fewer impressions than what their views are i.e., the recommendation algorithm directs trafﬁc towards other channel categories. A detailed breakdown of intra-category and crosscategory recommendations is presented by recommendations percentages in Figure 12 and by a number of impressions in Figure 13 in Appendix B show the strength of intra-category recommendations by channel.
We can see that the recommendation algorithm does have an intra-category preference, but this preference is dependent on the channel category. For example, 51 percent of trafﬁc from Center Left/MSM channels is directed to other channels belonging to the same category (see Figure 12). Also, the remaining recommendations are directed mainly to two categories: Partisan Left (18.2 percent) and Partisan Right (11 percent), both primarily consisting of mainstream media channels.
Figure 5 presents a simpliﬁed version of the recommendation ﬂows, highlighting the channel categories that beneﬁt from the recommendations trafﬁc. From this ﬁgure, we can observe that there is a signiﬁcant net ﬂow of recommendations towards channels that belong to the category Partisan Left. For example, the Social Justice category suffers from crosscategory recommendations. For viewers of channels that are categorized as Social Justice, the algorithm presents 5.9 more recommendations towards the Partisan Left channels than vice versa and another 5.2 million views per day towards Center/Left MSM channels. Figure 5 also shows a ”pipeline” that directs trafﬁc towards the Partisan Left category from other groups via the intermediary Center/Left MSM category. This is true even for the other beneﬁciary category, the Partisan Right, which loses 2.9 million recommendations to Partisan Left but beneﬁts with a net ﬂow of recommendations from different right-leaning categories (16.9M).
However, when it comes to categories that could be potentially radicalizing, this statement is only partially supported. Channels that we grouped into Conspiracy Theory or White Identitarian have very low percentages of recommendations within the group itself (as shown in 12). In contrast, channels that we categorized into Center/Left MSM or Partisan Left or Right have higher numbers for recommendations that remain within the group. These data show that a dramatic shift to more extreme content, as suggested by media [15] [30], is untenable.
Second, we posited that there is a C2 - Right-Wing Advantage, i.e., YouTube’s recommendation algorithm prefers rightwing content over other perspectives. This claim is also not supported by the data. On the contrary, the recommendation algorithm favors content that falls within mainstream media groupings. YouTube has stated that its recommendations are

Fig. 5. The Direction of Algorithmic Recommendations

based on content that individual users watch and engage in and that peoples’ watching habits inﬂuence 70 percent of recommendations.
Figure 6 shows the algorithmic advantage based on daily views. From this Figure, we can observe that the two out of the top three categories (Partisan Left, and Partisan Right) receive more recommendations than other categories irregardless of what category the seed channels belong to. Conversely, any other category does not get their channels suggested by the algorithm. In other words, the recommendation algorithm inﬂuences the trafﬁc from all channels towards Partisan Left and Partisan Right channels, regardless of what category the channel that the users viewed belonged to.
We can also observe this trend from a higher-level aggregate categorization, as is presented in Figure 7. The Figure afﬁrms that channels that present left or centrist political content are advantaged by the recommendation algorithm, while channels that present content on the right are at a disadvantage.
The recommendations algorithm advantages several groups to a signiﬁcant extent. For example, we can see that when one watches a video that belongs to the Partisan Left category, the algorithm will present an estimated 3.4M impressions to the Center/Left MSM category more than it does the other way. On the contrary, we can see that the channels that suffer the most substantial disadvantages are again channels that fall outside mainstream media. Both right-wing and left-wing YouTuber channels are disadvantaged, with White Identitarian and Conspiracy channels being the least advantaged by the algorithm. For viewers of conspiracy channel videos, there are 5.5 million more recommendations to Partisan Right videos than vice versa.
We should also note that right-wing videos are not the only disadvantaged groups. Channels discussing topics such as social justice or socialist view are disadvantaged by the recommendations algorithm as well. The common feature of

Fig. 6. Algorithmic Advantage by Groups
Fig. 7. High-level view of Algorithmic Advantages/Disadvantages in Recommendation Impressions
disadvantages channels is that their content creators are seldom broadcasting networks or mainstream journals. These channels are independent content creators.
When it comes to the third claim regarding YouTube’s potential C3 - Radicalization Inﬂuence, i.e., YouTube’s algorithm inﬂuences users by exposing them to more extreme content than they would otherwise; this claim is also not supported by our data. On the contrary, the recommendation algorithm appears to restrict trafﬁc towards extreme rightwing categories actively. The two most drastic examples are

channels we have grouped under the categories of White Identitarian and Conspiracy theory channels. These two groups receive almost no trafﬁc based on the recommendation algorithm, as presented in Figures 12 and 6.

Fig. 9. Trafﬁc from Conspiracy Channels

Fig. 8. Trafﬁc from White Identitarian Channels
Another way to visualize the lack of trafﬁc from recommendations is to view the recommendations’ ﬂow. Figures 8 and 9 show that the majority of the recommendations ﬂow to either towards Partisan Right, Center/Left MSM, and Partisan Left content. The White Identitarian channel trafﬁc is also directed towards Libertarian and, to a small extent, even towards centrist Anti-SJW content.
Besides, the Figure 2 showed that the daily views for White Identitarian channels are marginal. Even if we would compare the views of White Identitarian channels with the Conspiracy channels, we could see that Conspiracy channels are twice as viewed than content created by the White Identitarians. This discrepancy is notable since Conspiracy channels seem to gain zero trafﬁc from recommendations (as shown in Figure 12) and are the least advantaged group of all categories. While MRA (Men’s Rights Activists) channels form the smallest category in our study, the White Identitarian category is in the bottom ﬁve of all groups. Another comparison that illustrates the marginality of White Identitarian channels is the fact that this group consists of thirty-seven channels with enough views

to ﬁt within the scope of the study. The White Identitarian category includes almost the same number of channels as Libertarian channels but receives only a third as many views.
Our fourth claim stated that there exists C4 - Right-Wing Radicalization Pathway i.e., YouTube algorithm inﬂuences viewers of mainstream and center-left channels via increasingly left-wing critical content to the extreme right.” Again, these data suggest the opposite. The right-wing channel that beneﬁts the most from the recommendation algorithm is Fox News, a mainstream right-wing media outlet. Figure 10 shows that Fox News receives over 50 percent of the recommendations form other channels, which map to the category of the Partisan Right. Fox News also receives large numbers of recommendations from every other category that could be considered right-wing. This observation is aligned with the overall trend of the algorithm to beneﬁting mainstream media outlets over independent YouTube channels. Fox News is likely disproportionally favored on the right due to a lack of other right-leaning mainstream outlets, while trafﬁc in the Center/Left MSM and Partisan Left is more evenly distributed among their representative mainstream outlets.
We can also analyze the overall net beneﬁts the mainstream media channels are receiving from the algorithm by aggregating the mainstream channels into one high-level group and

while the data refute all the other three claims. Rejection of these claims seems to be in line with studies that critique the claims of YouTube’s algorithm as a pathway to radicalization [50].

TABLE II CLAIMS AND DATA SUPPORT

Claim C1 - Radical Bubbles. Recommendations inﬂuence viewers of radical content to watch more similar content than they would otherwise, making it less likely that alternative views are presented. C2 - Right-Wing Advantage. YouTube’s recommendation algorithm prefers right-wing content over other perspectives. C3 - Radicalization Inﬂuence. YouTube’s algorithm inﬂuences users by exposing them to more extreme content than they would otherwise. C4 - Right-Wing Radicalization Pathway. YouTube algorithm inﬂuences viewers of mainstream and center-left channels by recommending extreme right-wing content, content that aims to disparage left-wing or centrist narratives.

Data Support Partially supported
Not supported
Not supported
Not supported

Fig. 10. Algorithmic advantage for Fox News
independent YouTubers into another group and comparing the algorithmic advantages and disadvantages for each. The third group we separated from mainstream media and YouTubers is the group we called the ”Missing Link Media.” This group encompasses media outlets that have ﬁnancial backing with the traditional mainstream outlets but are not considered part of the conventional mainstream media. For example, left-wing channels such as Vox or Vice belong to this category, while BlazeTV is an equivalent for the right-leaning media. Figure 11 shows the clear advantage mainstream media channels receive over both independent channels and Missing Link Media channels.

YouTube has stated that its algorithm will favor more recent videos that are popular both in terms of views as well as engagement [38]. The algorithm will recommend more videos based on a user proﬁle, or the most current, popular videos for anonymous viewers. YouTube has stated that they are attempting to maximize the likelihood that a user will enjoy their recommended videos and will remain on the platform for as long as possible. The viewing history determines whether the algorithm will recommend the viewer more extreme content. Antithetical to this claim is that our data show that even if the user is watching very extreme content, their recommendations will be populated with a mixture of extreme and more mainstream content. YouTube is, therefore, more likely to steer people away from extremist content rather than vice versa.

V. LIMITATIONS AND CONCLUSIONS

Fig. 11. Algorithmic Advantage of Mainstream Media
Finally, based on the ﬁndings and analysis of our four claims, we conclude that these data offer little support to the claims that YouTube’s recommendation algorithm will recommend content that might be contributing to the radicalization of the user-base. Only the ﬁrst claim is partially supported,

There are several limitations to our study that must be considered for the future. First, the main limitation is the anonymity of the data set and the recommendations. The recommendations the algorithm provided were not based on videos watched over extensive periods. We expect and have anecdotally observed that the recommendation algorithm gets more ﬁne-tuned and context-speciﬁc after each video that is watched. However, we currently do not have a way of collecting such information from individual user accounts, but our study shows that the anonymous user is generally directed towards more mainstream content than extreme. Similarly, anecdotal evidence from a personal account shows that YouTube suggests content that is very similar to previously watched videos while also directing trafﬁc into more mainstream channels. That is, contrary to prior claims; the algorithm does not appear to stray into suggesting videos several degrees away from a user’s normal viewing habits.

Second, the video categorization of our study is partially subjective. Although we have taken several measures to bring objectivity into the classiﬁcation and analyzed similarities between each labeler by calculating the intraclass correlation coefﬁciencies, there is no way to eliminate bias. There is always a possibility for disagreement and ambiguity for categorizations of political content. We, therefore, welcome future suggestions to help us improve our classiﬁcation.
In conclusion, our study shows that one cannot proclaim that YouTube’s algorithm, at the current state, is leading users towards more radical content. There is clearly plenty of content on YouTube that one might view as radicalizing or inﬂammatory. However, the responsibility of that content is with the content creator and the consumers themselves. Shifting the responsibility for radicalization from users and content creators to YouTube is not supported by our data. The data shows that YouTube does the exact opposite of the radicalization claims. YouTube engineers have said that 70 percent of all views are based on the recommendations [38]. When combined with this remark with the fact that the algorithm clearly favors mainstream media channels, we believe that it would be fair to state that the majority of the views are directed towards left-leaning mainstream content.
We agree with the Munger and Phillips (2019), the scrutiny for radicalization should be shined upon the content creators and the demand and supply for radical content, not the YouTube algorithm. On the contrary, the current iteration of the recommendations algorithm is working against the extremists. Nevertheless, YouTube has conducted several deletion sweeps targeting extremist content [29]. These actions might be ill-advised. Deleting extremist channels from YouTube does not reduce the supply for the content [50]. These banned content creators migrate to other video hosting more permissible sites. For example, a few channels that were initially included in the Alt-right category of the Ribero et al. (2019) paper, are now gone from YouTube but still exist on alternative platforms such as the BitChute. The danger we see here is that there are no algorithms directing viewers from extremist content towards more centrist materials on these alternative platforms or the Dark Web, making deradicalization efforts more difﬁcult [51]. We believe that YouTube has the potential to act as a deradicalization force. However, it seems that the company will have to decide ﬁrst if the platform is meant for independent YouTubers or if it is just another outlet for mainstream media.
A. The Visualization and Other Resources
Our data, channel categorization, and data analysis used in this study are all available on GitHub for anyone to see. Please visit the GitHub page for links to data or the Data visualization. We welcome comments, feedback, and critique on the channel categorization as well as other methods applied in this study.

B. Publication Plan
This paper has been submitted for consideration at First
Monday.
C. Acknowledgments
First, we would like to thank our volunteer labeler for all the
hours spent on YouTube. We would also like to thank Cody
Moser, Brenton Milne and Justin Murphy and everyone else
who gave their feedback on the early drafts of this paper and
aided the editing.
REFERENCES
[1] P. Ferdinand, The Internet, democracy and democratization. Routledge, 2013.
[2] C. Blaya, “Cyberhate A review and content analysis of intervention strategies,” Aggression and Violent Behavior, vol. 45, pp. 163–172, 2019.
[3] B. Pfaffenberger, “” if i want it, it’s ok”: Usenet and the (outer) limits of free speech,” The Information Society, vol. 12, no. 4, pp. 365–386, 1996.
[4] J. M. Kayany, “Contexts of uninhibited online behavior: Flaming in social newsgroups on usenet,” Journal of the American Society for Information Science, vol. 49, no. 12, pp. 1135–1141, 1998.
[5] H. Berghel and D. Berleant, “The online trolling ecosystem,” Computer, no. 8, pp. 44–51, 2018.
[6] ITU, “World telecommunication/ict indicators database online,” International Telecommunication Union, 23rd Edition, http:// handle.itu.int/ 11. 1002/ pub/ 81377c7d-en, 2019.
[7] I. Gagliardone, D. Gal, T. Alves, and G. Martinez, Countering online hate speech. Unesco Publishing, 2015.
[8] A. Ben-David and A. Matamoros-Ferna´ndez, “Hate speech and covert discrimination on social media: Monitoring the facebook pages of extreme-right political parties in spain,” International Journal of Communication, vol. 10, pp. 1167–1193, 2016.
[9] P. Burnap and M. L. Williams, “Cyber hate speech on twitter: An application of machine classiﬁcation and statistical modeling for policy and decision making,” Policy & Internet, vol. 7, no. 2, pp. 223–242, 2015.
[10] E. Chandrasekharan, U. Pavalanathan, A. Srinivasan, A. Glynn, J. Eisenstein, and E. Gilbert, “You can’t stay here: The efﬁcacy of reddit’s 2015 ban examined through hate speech,” Proceedings of the ACM on HumanComputer Interaction, vol. 1, no. CSCW, p. 31, 2017.
[11] L. Knuttila, “User unknown: 4chan, anonymity and contingency,” First Monday, vol. 16, no. 10, 2011.
[12] A. Nagle, Kill all normies: Online culture wars from 4chan and Tumblr to Trump and the alt-right. John Hunt Publishing, 2017.
[13] S. Agarwal and A. Sureka, “Spider and the ﬂies: Focused crawling on tumblr to detect hate promoting communities,” arXiv preprint arXiv:1603.09164, 2016.
[14] Q. Shen, M. M. Yoder, Y. Jo, and C. P. Rose, “Perceptions of censorship and moderation bias in political debate forums,” in Twelfth International AAAI Conference on Web and Social Media, 2018.
[15] K. Roose, “The making of a youtube radical,” The New York Times (June 2019). https:// www.nytimes.com/ interactive/ 2019/ 06/ 08/ technology/ youtube-radical.html, 2019.
[16] ADL, “Despite youtube policy update, anti-semitic, white supremacist channels remain,” ADLs Center on Extremism, 2019.
[17] L. Munn, “Alt-right pipeline: Individual journeys to extremism online,” First Monday, vol. 24, no. 6, 2019.
[18] V. Andre, “neojihadism and youtube: Patani militant propaganda dissemination and radicalization,” Asian security, vol. 8, no. 1, pp. 27–53, 2012.
[19] I. Awan, “Cyber-extremism: Isis and the power of social media,” Society, vol. 54, no. 2, pp. 138–149, 2017.
[20] N. de Boer, H. Su¨tfeld, and J. Groshek, “Social media and personal attacks: A comparative perspective on co-creation and political advertising in presidential campaigns on youtube,” First Monday, vol. 17, no. 12, 2012.
[21] J. B. Schmitt, D. Rieger, O. Rutkowski, and J. Ernst, “Counter-messages as prevention or promotion of extremism?! the potential role of youtube: Recommendation algorithms,” Journal of Communication, vol. 68, no. 4, pp. 780–808, 2018.

[22] T. Gillespie, Custodians of the Internet: Platforms, content moderation,

2019).

https:// www.theverge.com/ 2019/ 9/ 12/ 20862696/

and the hidden decisions that shape social media. Yale University

pewdiepie-adl-donation-backlash-100-million-subscribers, 2019.

Press, 2018.

[46] J.-M. Eberl, H. G. Boomgaarden, and M. Wagner, “One bias ﬁts all?

[23] S. Agarwal and A. Sureka, “Topic-speciﬁc youtube crawling to detect

three types of media bias and their effects on party preferences,”

online radicalization,” in International Workshop on Databases in Net-

Communication Research, vol. 44, no. 8, pp. 1125–1148, 2017.

worked Information Systems. Springer, 2015, pp. 133–151.

[47] F. N. Ribeiro, L. Henrique, F. Benevenuto, A. Chakraborty, J. Kul-

[24] A. Sureka, P. Kumaraguru, A. Goyal, and S. Chhabra, “Mining youtube to discover extremist videos, users and hidden communities,” in Asia Information Retrieval Symposium. Springer, 2010, pp. 13–24.
[25] M. N. Hussain, S. Tokdemir, N. Agarwal, and S. Al-Khateeb, “Analyzing disinformation and crowd manipulation tactics on youtube,” in 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM). IEEE, 2018, pp. 1092–1095.
[26] M. Z. Asghar, S. Ahmad, A. Marwat, and F. M. Kundi, “Sentiment analysis on youtube: a brief survey,” arXiv preprint arXiv:1511.09142, 2015.
[27] Youtube, “Policies and safety,” https:// www.youtube.com/ about/ policies/ , accessed 7th of November 2019, 2019.
[28] Youtube, “Limited features for certain videos,” https:// support.google.

shrestha, M. Babaei, and K. P. Gummadi, “Media bias monitor: Quantifying biases of social media news outlets at large-scale,” in Twelfth International AAAI Conference on Web and Social Media, 2018. [48] D. V. Cicchetti, “Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology.” Psychological assessment, vol. 6, no. 4, p. 284, 1994. [49] M. J. Bannister, D. Eppstein, M. T. Goodrich, and L. Trott, “Forcedirected graph drawing using social gravity and scaling,” in International Symposium on Graph Drawing. Springer, 2012, pp. 414–425. [50] K. Munger and J. Phillips, “A supply and demand framework for youtube politics,” Preprint, 2019. [51] G. Hussain and E. M. Saltman, Jihad trending: A comprehensive analysis of online extremism and how to counter it. Quilliam, 2014.

com/ youtube/ answer/ 7458465?hl=en, accessed 7th of November 2019,

2019.

[29] P. Martienau, “Youtube removes more videos but still misses a

lot of hate,” The Wired (March 2019). https:// www.wired.com/ story/

youtube-removes-videos-misses-hate/ , 2019.

[30] Z. Tufekci, “Youtube, the great radicalizer,” The New York Times, vol. 10,

2018.

[31] J. R. Vacca, Online Terrorist Propaganda, Recruitment, and Radicaliza-

tion. CRC Press, 2019.

[32] M. H. Ribeiro, R. Ottoni, R. West, V. A. Almeida, and W. Meira,

“Auditing radicalization pathways on youtube,” arXiv preprint

arXiv:1908.08313, 2019.

[33] N. Agarwal, R. Gupta, S. K. Singh, and V. Saxena, “Metadata based

multi-labelling of youtube videos,” in 2017 7th International Conference

on Cloud Computing, Data Science & Engineering-Conﬂuence. IEEE,

2017, pp. 586–590.

[34] A. G. Greenwald, D. E. McGhee, and J. L. Schwartz, “Measuring

individual differences in implicit cognition: the implicit association test.”

Journal of personality and social psychology, vol. 74, no. 6, p. 1464,

1998.

[35] P. S. Forscher, C. K. Lai, J. R. Axt, C. R. Ebersole, M. Herman, P. G.

Devine, and B. A. Nosek, “A meta-analysis of procedures to change

implicit measures.” Journal of personality and social psychology, 2019.

[36] R. Ottoni, E. Cunha, G. Magno, P. Bernardina, W. Meira Jr, and

V. Almeida, “Analyzing right-wing youtube channels: Hate, violence

and discrimination,” in Proceedings of the 10th ACM Conference on

Web Science. ACM, 2018, pp. 323–332.

[37] P. J. Moor, A. Heuvelman, and R. Verleur, “Flaming on youtube,”

Computers in human behavior, vol. 26, no. 6, pp. 1536–1546, 2010.

[38] Z. Zhao, L. Hong, L. Wei, J. Chen, A. Nath, S. Andrews, A. Kumthekar,

M. Sathiamoorthy, X. Yi, and E. Chi, “Recommending what video to

watch next: a multitask ranking system,” in Proceedings of the 13th

ACM Conference on Recommender Systems. ACM, 2019, pp. 43–51.

[39] ChannelCrawler, “The youtube channel crawler,” https:// channelcrawler.

com/ , 2019.

[40] SocialBlade, “Top 25 youtube users tagged with politics sorted by video

views,” https:// socialblade.com/ youtube/ top/ tag/ politics/ videoviews ac-

cessed 7th of November 2019, 2019.

[41] Feedspot, “Politicial youtube channels,” https:// blog.feedspot.com/

political youtube channels/ , 2019.

[42] S. H. Lee, P.-J. Kim, and H. Jeong, “Statistical properties of sampled

networks,” Physical review E, vol. 73, no. 1, p. 016102, 2006.

[43] M. Thiessen, “The southern poverty law center has lost

all

credibility,”

https:// www.washingtonpost.com/ opinions/

the-southern-poverty-law-center-has-lost-all-credibility/ 2018/ 06/

21/ 22ab7d60-756d-11e8-9780-b1dd6a09b549 story.html , accessed 7

November 2019, 2018.

[44] B. Mandel, “The anti-defamation leagues sad slide

into just another left-wing pressure group,” The Fed-

eralist (July 2017). https:// thefederalist.com/ 2017/ 07/ 28/

anti-defamation-leagues-sad-slide-just-another-left-wing-pressure-group/ ,

2019.

[45] J. Alexander, “Pewdiepie pulls 50,000 pledge to jewish

anti-hate group after fan backlash,” The Verge (September

APPENDIX A CHANNEL CATEGORIZATION
A. Channel Views and Formulas
We have used several formulas in order to capture the ﬂow of recommendations. The main concept in our study is the impression. Impression is an estimate for the number of times a viewer was presented with a recommendation. We count each of the top 10 recommendations for a video as an ”impression”. Only YouTube knows true impressions, so we use the following process create an estimate:

Tag Impressions
Relevant impressions Channel views Daily channel views Relevant channel views

Examples An estimate for the number of times a viewer was presented with a recommendation. I.e. we count each of the top 10 recommendations for a video as an ”impression”. Only YouTube knows true impressions, so we use the following process create an estimate: Consider each combination of videos (e.g. Video A to Video B) (A to B impressions) = (recommendations from A to B) / (total recommendations from Video A) x (*A’s views) x (recommendations per video = 10) (A channel’s relevance %) x impressions
The total number of video views since ﬁrst of January 2018 (channel views) * (days in the period videos have been recorded for the channel) (daily channel views) * (channel relevance %)

• Everything else → Unclassiﬁed

C. Hard Tags

Hard tags are tags sources from external sources. Any combination of the following tags can be applied to a channel. Hard tags are for comparison between the categorization presented in this paper and other work, academic or otherwise, and also used to distinguish between YouTubers and TV or other mainstream media content.

Tag Mainstream News Reporting on newly received or noteworthy information. Widely accepted and selfidentiﬁed as news (even if mostly opinion). Appears in either https://www.adfontesmedia.com or https://mediabiasfactcheck.com. To tag they should have ¿ 30% focus on politics & culture. TV Content originally created for broadcast TV or cable Ribeiro et al.’s alt-lite, alt-right, IDW

Examples Fox News, Buzzfeed News

CNN, Vice

As

listed

in

Auditing

Radicalization

Pathways on

YouTube [32]

B. Tag Aggregation
In order to create meaningful ideological categories, we have aggregated the tags assigned for each channel. In order to calculate the majority view, each soft tag is assessed independently. For each tag, the number of the reviewer with that rag must tally to more than half. Eighteen categories of soft tags, the soft tags deﬁning left, center, and right, and the hard tags deﬁning the media type, were aggregated for the visualization and data analysis. The following list informs which tags or tag combinations were aggregated to represent an ideology, rather than just a collection of tags.
• White Identitarian → White Identitarian • MRA → MRA • Conspiracy → Conspiracy • Libertarian → Libertarian • AntiSJW and either Provocateur or PartisanRight →
Provocative Anti-SJW • AntiSJW → Anti-SJW3 • Socialist → Socialist • ReligiousConservative → Religious Conservative • Social Justice or Anti-Whiteness → Social Justice • Left or Center ’hard’ tag and Mainstream News or
Missing Link Media ’hard’ tag → Center/Left MSM • PartisanLeft → Partisan Left • PartisanRight → Partisan Right • AntiTheist → Anti-Theist
3This group has a signiﬁcant overlap with the intellectual dark web-group as described by Ribero et al. (2019), Munger and Phillips (2019)

D. Soft Tags
Soft tags are a natural category for US YouTube content. Many traditional ways of dividing politics are not natural categories that would accurately describe the politics of YouTube channels. In general, YouTubers are providing reaction and sensemaking on other channels or current events in the United States. We have created a list of categories that attempt to align the stands taken by the channels more naturally, expanding the categorization beyond the left, center, and right categories.
The tag needs to be engaging in some way to the current meta-discussion about YouTube’s inﬂuence on politics. Our list of categories intends to cover major cultural topics and label channels to the best of our abilities. We have tried to ﬁnd speciﬁc positions that could be mixed and aggregate in order to create categories that would represent ideologies.
Our guiding principle is that, in order to apply one of these tags, one should be able to judge the channel by the channel content itself. It is important not to rely on an outside judgment about the channel’s content. It is also important to interpret the content with full context: there should be no mind-reading and no relying on a judgment from other sources. There should also be enough channels per each category. If the category is too niche, it should be excluded, unless it is essential for the radicalization pathway theory.

Tag Conspiracy A channel that regularly promotes a variety of conspiracy theories. A conspiracy theory explains an event/circumstance as the result of a secret plot that is not widely accepted to be true (even though sometimes it is). Example conspiracy theories:
• Moon landings were faked • QAnon & Pizzagate • Trump colluding with Russia to win the election

Examples X22Report, The Next News Network

Libertarian A political philosophy that has liberty as its main principle. Generally skeptical of authority and state power (e.g., regulation, taxes, government programs). Favors free markets and private ownership. Note: To tag someone, this should be the primary driver of their politics. Does not include libertarian socialists who also are anti-state but are anti-capitalist and promote communal living. Anti-SJW Channel has to have a signiﬁcant focus on criticizing ”Social Justice” (see next category) with a positive view of the marketplace of ideas and discussing controversial topics. To tag a channel, this should be a common focus in their content. Social Justice The channel promotes
• Identity Politics & Intersectionality narratives of oppression though the combination of historically oppressed identities: Women, Non-whites, Transgender
• Political Correctness the restriction of ideas and words you can say in polite society. • Social Constructionism the idea that the differences between individuals and groups are explained entirely
by the environment. For example, sex differences are caused by culture, not by biological sex.
The channel content is often in reaction to Anti-SJW or conservative content rather than purely a promotion of social justice ideas. The supporters of the content creator are active on Reddit in subreddit called r/Breadtube, and the creators often identify with this label. This tag only includes breadtuber’s if their content is criticizing anti-SJW’s (promoting socialism is its own, separate tag). White Identitarian Identiﬁes-with/is-proud-of the superiority of ”whites” and Western Civilization. An example of identifying with ”western heritage” would be to refer to the Sistine chapel or Bach as ”our culture.” Often will promote
• An ethnostate where residence or citizenship would be limited to ”whites” OR a type of nationalist that seek to maintain a white national identity (white nationalism)
• A historical narrative focused on the ”white” lineage and its superiority • Essentialist concepts of racial differences
The content creators are very concerned about whites becoming a minority population in the US/Europe (the Great Replacement - theory) Educational Channel that mainly focuses on education material, of which over 30% is focused on making sense of culture or politics. Late Night Talk shows Channel with content presented humorous monologues about the day’s news, guest interviews, and comedy sketches. To tag, they should have over 30% focus on politics & culture. Partisan Left Channel mainly focused on politics and exclusively critical of Republicans. Would agree with this statement: ”GOP policies are a threat to the well-being of the country.” Partisan Right Channel mainly focused on politics and exclusively critical of Democrats. Must support Trump. Would agree with this statement: ”Democratic policies threaten the nation.” Anti-theist The self-identiﬁed atheist who is also actively critical of religion. Also called New Atheists or Street Epistemologists. Usually combined with an interest in philosophy. Religious Conservative A channel with a focus on promoting Christianity or Judaism in the context of politics and culture. Socialist (Anti-Capitalist) Focus on the problems of capitalism. Endorse the view that capitalism is the source of most problems in society. Critiques of aspects of capitalism that are more speciﬁc (i.e., promotion of fee healthcare or a large welfare system or public housing) don’t qualify for this tag. Promotes alternatives to capitalism. Usually, some form of either Social Anarchist (stateless egalitarian communities) or Marxist (nationalized production and a way of viewing society through class relations and social conﬂict). Revolutionary Endorses the overthrow of the current political system. For example, many Marxist and Ethnonationalists are revolutionaries because they want to overthrow the current system and accept the consequences. Provocateur Enjoys offending and receiving any kind of attention (positive or negative). Takes extreme positions, or frequently breaks cultural taboos. Often it is unclear if they are joking or serious. MRA (Mens Rights Activist) Focus on advocating for rights for men. See men as the oppressed sex and will focus on examples where men are currently oppressed. Incels, who identify as victims of sex inequality, would also be included in this category. Missing Link Media Channels funded by companies or venture capital, but not large enough to be considered ”mainstream.” They are generally accepted as more credible than independent YouTube content. State Funded Channels that are funded by governments. Anti-Whiteness A subset of Social Justice that, in addition to intersectional beliefs about race, has a signiﬁcant portion of content that essentializes race and disparages ”whites” as a group. Channel should match most of the following:
• Negative generalization about ”whites”. E.g. ”White folks are unemotional, they hardly even cry at funerals,” e.g., How To Play The Game w/WS 5 Daily Routines
• Use of the word ”whiteness” as a slur, or an evil force. e.g., ”I try to be less white” (Robin DiAngelo) • Simplistic narratives about American history, where the most important story is of slavery and racism. • Dilute terms like racism or white supremacy so that they include most Americans while keeping the stigma
and power of the word. • content exclusively framing current events into racial oppression. Usually in the form of police violence
against blacks, x-while-black (e.g., swimming while black, walking while black)...

Reason, John Stossel, The Cato Institute Sargon of Akkad, Tim Pool Peter Cofﬁn, hbomberguy
NPIRADIX (Richard Spencer), Stefan Molyneux
TED, SoulPancake Last Week Tonight, Trevor Noah The Young Turks, CNN Fox News, Candace Owens Sam Harris, CosmicSkeptic, Matt Dillahunty Ben Shapiro, PragerU BadMouseProductions, NonCompete
Libertarian Socialist Rants, Jason Unruhe StevenCrowder, MILO Karen Straughan Vox, NowThis News PBS NewsHour, Al Jazeera, RT African Diaspora News Channel

APPENDIX B DETAILED ALGORITHMIC ADVANTAGES AND
DISADVANTAGES
We discuss algorithmic advantages and disadvantages at the higher level in Section IV. This appendix presents two additional ﬁgures that shows a breakdown of recommendation algorithm trafﬁc channel by channel.
First, Figure 12 presents the relative portion of recommendations between groups. The diagonal column cutting across the chart shows the percentages of intra-category recommendations, i.e., the percentage of recommendations that are directed to the same category. In contrast, lower percentages in this diagonal that the majority of the trafﬁc is directed outwards from the category. The other cells show the percentages each group is recommended in relation to other categories. For example, if one is to view a video that belongs to the Provocative AntiSJW category, the bulk of the recommendations will suggest videos that belong to either Partisan Right or non-political channels.
The non-political channels in this chart are channels that fall outside our labeled data categories. The Figure 12 illustrates that these channels are recommended in large numbers for categories that fall on the fringes, such as the White Identitarian and MRA channels, directing the trafﬁc towards less contentious material.

are at the bottom. Categories in darkest shades of blue are most advantaged, whereas the categories on darker shades of red are at least advantage.
Fig. 13. Algorithmic Advantages/Disadvantages in Recommendation Impressions
Categories in grey are also at a disadvantage, but to a lesser extent than the categories in red the small arrows in the image point towards the category, which is beneﬁting from the recommendations algorithm. Arrows are pointing towards the group that receives more recommendations that it is given by the algorithm, i.e., pointing towards the group, which is advantaged.
...

Fig. 12. Cross-category and Intra-category Recommendations
Figure 12 presents the different advantages and disadvantages each group has due to the recommendation system in more detail. The Figure compares the daily net ﬂow of recommendations for each group. The categories in the Figure are organized based on their algorithmic advantage, the most advantaged groups are at the top, and least advantaged groups

